# ğŸ¨ğŸ” CLIP Multimodal Image Search

Welcome to **CLIP Multimodal Image Search** â€” a sleek Python tool that leverages OpenAI's **CLIP** model to match images with text queries! ğŸš€  
Harness the power of multimodal AI to find the best matching images based on your textual input. Perfect for exploring semantic similarity across vision and language! ğŸ§ âœ¨

---

## âš¡ Features

- ğŸ”¥ Uses state-of-the-art **CLIP (ViT-B/32)** model for embedding images and text  
- ğŸ–¼ï¸ Batch processes multiple images effortlessly  
- ğŸ“ Calculates cosine similarity to find top matching images  
- ğŸ¯ Easily extendable to support top-k matches and more image formats  
- ğŸ’» GPU accelerated if available for faster processing

---

## ğŸ› ï¸ Requirements

- Python 3.7+  
- [PyTorch](https://pytorch.org/)  
- [OpenAI CLIP](https://github.com/openai/CLIP)  
- [Pillow](https://python-pillow.org/)  

Install all dependencies with:  
bash
pip install -r requirements.txt

## ğŸš€ Usage

1. Add your images (JPG or PNG) to the pics/ folder ğŸ“‚
2. Modify the text_query variable in the script with your search phrase âœï¸
3. Run the script and watch magic happen:
python find_top3_images.py

Get the best matching images based on your query â€” easy and fast! âš¡

## ğŸ“ Project Structure

clip-multimodal-search/
â”œâ”€â”€ pics/                  # Sample images folder  
â”œâ”€â”€ find_top3_images.py    # Main CLIP search script  
â”œâ”€â”€ requirements.txt       # Python dependencies  
â””â”€â”€ README.md              # This awesome doc!  



